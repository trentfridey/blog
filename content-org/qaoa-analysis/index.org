#+TITLE: Using autograd in Python for Quantum Computing 
#+AUTHOR: Trent Fridey
#+DATE: 2021-01-31
#+HUGO_BASE_DIR: ~/trent/blog
#+HUGO_SECTION: posts/qaoa-autograd
#+HUGO_TAGS: quantum-computing python autodiff
#+LATEX_HEADER: \usepackage{svg}
#+LATEX_HEADER: \usepackage{tikz}

I recently participated in a challenge in quantum computing posed by the [[https://qosf.org][Quantum Open Source Foundation]].
In this post, I recap the problem description and my solution to the challenge. 
The challenge involves exploring the ability of a certain class of quantum circuits in preparing a desired four-qubit quantum state.
In this challenge, the problem is to optimize a quantum circuit composed of layers of parameterized gates.
To solve this, we implement the circuit in Python and optimize the parameters using the [[https://github.com/HIPS/autograd][autograd]] library.

* Goal

  The cost function we are going to minimize is:

  \[
  \varepsilon = \min_{\theta}\left||\psi(\theta)\rangle - |\phi\rangle \right|
  \]

  where $|\psi(\theta)\rangle$ is the state of the system after going through the circuit and the initial state is $|\psi\rangle = |0000\rangle$.

  We choose the state $|\phi\rangle$ to be the 4-qubit [[https://en.wikipedia.org/wiki/Greenberger%E2%80%93Horne%E2%80%93Zeilinger_state][GHZ]] state:

  \[
    \left|\phi\right\rangle = \frac{1}{\sqrt{2}} \left(\left|1111\right\rangle + \left|0000\right\rangle\right)
  \]

* Circuit Description
  
The circuit is composed of a variable number of layers of blocks. There are only two types of blocks &mdash; odd and even &mdash; and they have parameters we can tweak to prepare the given state.
  The blocks are arranged [[https://arxiv.org/abs/1709.03489][QAOA]]-style, which means each layer has the even block following the odd block.


  [[file:images/circuit_drawing.svg]]

The even blocks are composed of the following gates:

#+HEADER: :results none
#+HEADER: :imagemagick yes
#+HEADER: :fit yes :imoutoptions -geometry 400 :iminoptions -density 600
#+HEADER: headers '("\\usepackage{lmodern}")
#+BEGIN_SRC latex
  \begin{tikzpicture}[font=\selectfont]
  %
  % `operator' will only be used by Hadamard (H) gates here.
  % `phase' is used for controlled phase gates (dots).
  % `surround' is used for the background box.
  \tikzstyle{operator} = [draw,fill=white,minimum size=1.5em] 
  \tikzstyle{phase} = [fill,shape=circle,minimum size=5pt,inner sep=0pt]
  \tikzstyle{surround} = [fill=blue!10,thick,draw=black,rounded corners=2mm]
  %
  % Qubits
  \node at (0,0) (q1)  {$q_1$};
  \node at (0,-1) (q2) {$q_2$};
  \node at (0,-2) (q3) {$q_3$};
  \node at (0,-3) (q4) {$q_4$};
  %
  %
  % Column 2
  \node[phase] (phase11) at (2,0) {} edge [-] (q1);
  \node[phase] (phase12) at (2,-1) {} edge [-] (q2);
  \draw[-] (phase11) -- (phase12);
  %
  % Column 3
  \node[phase] (phase21) at (3,0) {} edge [-] (phase11);
  \node[phase] (phase22) at (3,-2) {} edge [-] (q3);
  \draw[-] (phase21) -- (phase22);
  % Column 4
  \node[phase] (phase31) at (4,0) {} edge [-] (phase21);
  \node[phase] (phase32) at (4,-3) {} edge [-] (q4);
  \draw[-] (phase31) -- (phase32);
  % Column 5
  \node[phase] (phase41) at (5,-1) {} edge [-] (phase12);
  \node[phase] (phase42) at (5,-2) {} edge [-] (phase22);
  \draw[-] (phase41) -- (phase42);
  % Column 6
  \node[phase] (phase51) at (6,-1) {} edge [-] (phase41);
  \node[phase] (phase52) at (6,-3) {} edge [-] (phase32);
  \draw[-] (phase51) -- (phase52);
  % Column 7
  \node[phase] (phase61) at (7,-2) {} edge [-] (phase42);
  \node[phase] (phase62) at (7,-3) {} edge [-] (phase52);
  \draw[-] (phase61) -- (phase62);
  % Column 8
  \node[operator] (op71) at (8,0)  {$R_z(\theta_1)$} edge [-] (phase31);
  \node[operator] (op72) at (8,-1) {$R_z(\theta_2)$} edge [-] (phase51);
  \node[operator] (op73) at (8,-2) {$R_z(\theta_3)$} edge [-] (phase61);
  \node[operator] (op74) at (8,-3) {$R_z(\theta_4)$} edge [-] (phase62);

  % Column 8
  \node (end1) at (9,0)  {} edge [-]  (op71);
  \node (end2) at (9,-1) {} edge [-] (op72);
  \node (end3) at (9,-2) {} edge [-] (op73);
  \node (end4) at (9,-3) {} edge [-] (op74);
  %
  %
  \end{tikzpicture}
#+END_SRC

#+ATTR_HTML: :width 80% 
[[file:images/even_block.svg]]

In mathematical notation:

\[
U_i^{\text{even}}(\theta_{i,n}) = \left(\prod_{\langle j,k \rangle}CZ(j,k) \bigotimes_{n=1}^4 R_{z}(\theta_{i,n}) \right)
\]

The odd blocks are composed of the rotation-X gates:

#+HEADER: :results none 
#+HEADER: :imagemagick yes
#+HEADER: :fit yes :imoutoptions -geometry 400 :iminoptions -density 600
#+BEGIN_SRC latex
  \begin{tikzpicture}[thick]
  %
  % `operator' will only be used by Hadamard (H) gates here.
  % `phase' is used for controlled phase gates (dots).
  % `surround' is used for the background box.
  \tikzstyle{operator} = [draw,fill=white,minimum size=1.5em] 
  \tikzstyle{phase} = [fill,shape=circle,minimum size=5pt,inner sep=0pt]
  \tikzstyle{surround} = [fill=blue!10,thick,draw=black,rounded corners=2mm]
  %
  % Qubits
  \node at (0,0) (q1)  {$q_1$};
  \node at (0,-1) (q2) {$q_2$};
  \node at (0,-2) (q3) {$q_3$};
  \node at (0,-3) (q4) {$q_4$};
  %
  % Column 1
  \node[operator] (op11) at (1,0)  {$R_x(\theta_1)$} edge [-] (q1);
  \node[operator] (op12) at (1,-1) {$R_x(\theta_2)$} edge [-] (q2);
  \node[operator] (op13) at (1,-2) {$R_x(\theta_3)$} edge [-] (q3);
  \node[operator] (op14) at (1,-3) {$R_x(\theta_4)$} edge [-] (q4);

  % Column 2
  \node (end1) at (2,0)  {} edge [-] (op11);
  \node (end2) at (2,-1) {} edge [-] (op12);
  \node (end3) at (2,-2) {} edge [-] (op13);
  \node (end4) at (2,-3) {} edge [-] (op14);
  %
  \end{tikzpicture}
#+END_SRC

#+ATTR_HTML: :width 20%
[[file:images/odd_block.svg]]

In mathematical notation:

\[
U_i^{\text{odd}}(\theta_{i,n}) = \left( \bigotimes_{n=1}^{4} R_{x}(\theta_{i,n}) \right)
\]

The operator for the $\ell^{\text{th}}$ layer is:

\[
U_{\ell} = U^{\text{even}}_{2\ell} U^{\text{odd}}_{\ell}
\]

* Implementation

  In the Python code below, we calculate $\varepsilon$ by constructing the unitary operator for the circuit with a specified number of layers (~n_layers~), and then we use the ~autograd~ library to find the minimum.

  But first we have define our gates:
  
  #+begin_src python :session
    import autograd.numpy as np
    from functools import reduce

        def Rx(theta):
          return np.array([[np.cos(0.5*theta),-1j*np.sin(0.5*theta)],
                           [-1j*np.sin(0.5*theta),np.cos(0.5*theta)]])

        def Rz(theta):
          return np.array([[np.cos(-0.5*theta) + 1j*np.sin(-0.5*theta),0],
                           [0,     np.cos(0.5*theta) + 1j*np.sin(theta) ]])

        def CZ(i,j):
            ops0 = [np.eye(2) for i in range(4)]
            ops1 = [np.eye(2) for i in range(4)]
            ops0[i] = p0
            ops1[i] = p1
            ops1[j] = sz
            cz0 = reduce(lambda res, op: np.kron(res, op), ops0)
            cz1 = reduce(lambda res, op: np.kron(res, op), ops1)
            return cz0 + cz1
  #+end_src 

  # Should I explain how the CZ gate is implemented?

  From these basic gates we can create the blocks:
  
  #+begin_src python :session
     def odd(thetas):
       rot = Rx(thetas[0])
       rots = [Rx(theta) for theta in thetas[1:]]
       for r in rots:
         rot = np.kron(rot, r)
       return rot

    def even(thetas):
      rot = Rx(thetas[0])
      rots = [Rz(theta) for theta in thetas[1:]]
      for r in rots:
        rot = np.kron(rot, r)
      CZs = np.eye(16)
      for i in range(3):
        for j in range(i+1, 4):
          CZs = CZ(i,j) @ CZs
      return CZs @ rot
  #+end_src

 And from the blocks, we can create the layers:

 #+begin_src python :session
  def layer(thetas):
    return even(thetas[4:8]) @ odd(thetas[0:4]) 
 #+end_src

 and define the circuit:

 #+begin_src python :session
  def circuit(thetas):
    circuit = np.eye(16)
    for l in range(n_layers):
      circuit = layer(thetas[8*l:8*(l+1)]) @ circuit
    return circuit
 #+end_src

 To setup the minimization, we need to specify an objective function (i.e. $\varepsilon$) to auto-differentiate.
 
 The following code block loads the ~autograd~ library, encodes our cost function in the ~objective~ function and computes the AD gradient. 

#+begin_src python :session
  from autograd import grad

  start_state = np.array([1] + [0 for i in range(1,16)])
  target = 0.5*np.sqrt(2) *(np.array([1] + [0 for i in range(1,15)] + [1]))

  def objective(thetas,iter):
    circ = circuit(thetas)
    normalization = np.sum(np.abs(np.dot(circ, start_state)))
    normalized_final_state = np.abs(np.dot(circ, start_state)/normalization
    return np.sum(normalized_final_state - target)

  grad_obj = grad(objective)
#+end_src

To implement the minimization, we need to specify a few things:

- a minimizer (here we choose [[https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam][~adam~]])
- the initial values for the parameters $\theta_{i,n}$:
  I use samples from the normal distribution $N(0,\pi^2)$  
- the step size and the number of iterations: 
 I chose a step size of 0.1 and at least 400 iterations. The trade-off here is speed vs. accuracy.
 
We call the minimizer by passing these configuration options to ~adam~. It will return us the optimized values for $\theta_{i,n}$, which we then use to compute $\varepsilon$ by passing them to the ~objective~ function.

#+begin_src python :session
  # Optimization
  init_thetas = np.random.normal(0, np.pi, 8*n_layers)
  step_size = 0.1
  num_iters = 400 + 20*n_layers

  optimized_thetas = adam(grad_obj, init_thetas, step_size=step_size, num_iters=num_iters, callback=handle_step)

  epsilon = objective(optimized_thetas,0)
#+end_src

For debugging purposes, we also pass a ~callback~ to our minimization routine -- here's the definition of ~handle_step~, which prints the objective every 20 steps:

#+begin_src python :session
  def handle_step(params, iter, grad):
    if iter % 20 == 0:
      print("Cost after {} steps is {}".format(iter, objective(params, iter)))
#+end_src

After running this with over a few layers and iterations, we can check the optimal value of our cost function $\varepsilon$ via a plot.
In the figure below, we have plotted the ranges of values of $\varepsilon$ that the ~adam~ optimizer returns, for one, two, three, and four layers respectively.
Since we are using a stochastic algorithm, the results are not the same every time, so we use a boxplot to see the distribution of results for each layer.

[[file:images/results.png]]

Sanity check: /does the range of values for $\varepsilon$ make sense?/

Well, our cost function involves the magnitude of the difference of two normalized vectors.
The maximum difference of two normalized vectors is 2, when they are pointing in exactly opposite directions.
The minimum difference would be 0, when they are perfectly aligned.
Since the plot shows results between 0.4 and 1.6, we can conclude that the results are not totally out-of-the-question.

From this plot, we can see that, for a small number of layers, more layers yields a better result.
This means that we can take the results of the optimization for ~n_layers = 4~, and plug them into our quantum circuit, and approximate the GHZ state!
